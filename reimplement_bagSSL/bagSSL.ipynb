{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100 80GB PCIe MIG 7g.80gb\n",
      "Number of GPUs available: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "print(\"Number of GPUs available:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet34\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import os\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset  # Import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagSSLModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        super(BagSSLModel, self).__init__()\n",
    "        self.encoder = resnet34(pretrained=False)\n",
    "        self.encoder.fc = nn.Identity()  # Remove the classification head\n",
    "\n",
    "        # Projector: 2-layer MLP\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(512, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)  # Extract features from ResNet-34\n",
    "        projections = self.projector(features)  # Map to embedding space\n",
    "        return projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def contrastive_loss(z1, z2, temperature=0.07):\n",
    "    # Normalize embeddings\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "\n",
    "    # Compute similarity matrix\n",
    "    similarity_matrix = torch.cat([z1, z2]).mm(torch.cat([z1, z2]).T) / temperature\n",
    "\n",
    "    # Positive pairs on diagonal\n",
    "    batch_size = z1.size(0)\n",
    "    labels = torch.arange(batch_size).cuda()\n",
    "    logits = similarity_matrix[:batch_size, batch_size:]\n",
    "\n",
    "    # Compute cross-entropy loss\n",
    "    return F.cross_entropy(logits, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/synaderi\n"
     ]
    }
   ],
   "source": [
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoints will be saved in: /home/synaderi/checkpoints\n"
     ]
    }
   ],
   "source": [
    "#checkpoint_dir = \"/home/synaderi/checkpoints\"\n",
    "#os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "#print(f\"Checkpoints will be saved in: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: /home/synaderi/checkpoints\n"
     ]
    }
   ],
   "source": [
    "# Define checkpoint directory\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(f\"Created directory: {checkpoint_dir}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {checkpoint_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/synaderi/mycondaenvs/my_pytorch_env/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/synaderi/mycondaenvs/my_pytorch_env/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, optimizer, and scheduler\n",
    "model = BagSSLModel(embedding_dim=128).cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.3, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "# Define scheduler for cosine decay\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=600)  # T_max: Total number of epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageOps\n",
    "\n",
    "def solarize(img, threshold=128):\n",
    "    \"\"\"\n",
    "    Apply solarization to a PIL image. Inverts all pixel values above the threshold.\n",
    "    Args:\n",
    "        img (PIL.Image.Image): Input image in PIL format.\n",
    "        threshold (int): Pixel value threshold for solarization.\n",
    "    Returns:\n",
    "        PIL.Image.Image: Solarized image.\n",
    "    \"\"\"\n",
    "    return ImageOps.solarize(img, threshold=threshold)\n",
    "\n",
    "\n",
    "# Define augmentations for T\n",
    "augmentations_T = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=32, scale=(0.08, 1.0), ratio=(3/4, 4/3)),  # Random crop\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Flip probability = 0.5\n",
    "    transforms.RandomApply(\n",
    "        [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
    "        p=0.8\n",
    "    ),  # Color jittering\n",
    "    transforms.RandomGrayscale(p=0.2),  # Color dropping (grayscale conversion)\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),  # Gaussian blur, probability 1.0\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])  # CIFAR-10 normalization\n",
    "])\n",
    "\n",
    "# Define augmentations for T'\n",
    "augmentations_T_prime = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=32, scale=(0.08, 1.0), ratio=(3/4, 4/3)),  # Random crop\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Flip probability = 0.5\n",
    "    transforms.RandomApply(\n",
    "        [transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1)],\n",
    "        p=0.8\n",
    "    ),  # Color jittering\n",
    "    transforms.RandomGrayscale(p=0.2),  # Color dropping (grayscale conversion)\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))], p=0.1),  # Gaussian blur, probability 0.1\n",
    "    transforms.RandomApply([transforms.Lambda(lambda img: solarize(img, threshold=128))], p=0.2),  # Solarization\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])  # CIFAR-10 normalization\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/synaderi/mycondaenvs/my_pytorch_env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Custom Dataset for BYOL\n",
    "class BYOLDataset(Dataset):\n",
    "    def __init__(self, dataset, transform1, transform2):\n",
    "        self.dataset = dataset  # Original CIFAR-10 dataset\n",
    "        self.transform1 = transform1  # Augmentation pipeline T\n",
    "        self.transform2 = transform2  # Augmentation pipeline T'\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, _ = self.dataset[index]\n",
    "        image1 = self.transform1(image)  # Apply T\n",
    "        image2 = self.transform2(image)  # Apply T'\n",
    "        return image1, image2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# Create BYOL dataset and DataLoader\n",
    "train_dataset = BYOLDataset(\n",
    "    CIFAR10(root=\"./data\", train=True, download=True, transform=None),\n",
    "    augmentations_T, augmentations_T_prime\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: ./checkpoints/epoch_100.pth\n",
      "Resuming training from epoch 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1676278/2560220383.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(latest_checkpoint)\n"
     ]
    }
   ],
   "source": [
    "# Specify the checkpoint path\n",
    "checkpoint_dir = \"./checkpoints\"\n",
    "latest_checkpoint = None\n",
    "\n",
    "# Find the latest checkpoint file\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth\")]\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = os.path.join(checkpoint_dir, sorted(checkpoints, key=lambda x: int(x.split('_')[1].split('.')[0]))[-1])\n",
    "\n",
    "# Load the checkpoint\n",
    "start_epoch = 0\n",
    "if latest_checkpoint:\n",
    "    print(f\"Loading checkpoint: {latest_checkpoint}\")\n",
    "    checkpoint = torch.load(latest_checkpoint)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print(f\"Resuming training from epoch {start_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [101/600], Loss: 1.8201\n",
      "Epoch [102/600], Loss: 1.8321\n",
      "Epoch [103/600], Loss: 1.8379\n",
      "Epoch [104/600], Loss: 1.8217\n",
      "Epoch [105/600], Loss: 1.8086\n",
      "Epoch [106/600], Loss: 1.7929\n",
      "Epoch [107/600], Loss: 1.7805\n",
      "Epoch [108/600], Loss: 1.7886\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8720/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(start_epoch, 600):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for images1, images2 in train_loader:\n",
    "        # Move to GPU\n",
    "        images1 = images1.cuda()\n",
    "        images2 = images2.cuda()\n",
    "\n",
    "        # Forward pass\n",
    "        z1 = model(images1)\n",
    "        z2 = model(images2)\n",
    "        loss = contrastive_loss(z1, z2)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/600], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Save checkpoint every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint_path = f\"./checkpoints/epoch_{epoch + 1}.pth\"\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': total_loss\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_pytorch_env)",
   "language": "python",
   "name": "my_pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
